Configuration:
The FDSTTP model requires relatively modest hardware resources for real-time inference, primarily due to its use of model compression techniques such as Tucker decomposition. For GPUs, users should use devices with at least 4GB of video memory, such as the NVIDIA GTX 1660 Ti or RTX 3060, to support parallel computation of tree and temporal convolutions. Using a GPU that supports FP16 or INT8 quantization can further reduce video memory usage and improve inference speed. For CPUs, a processor with at least 4 cores and a clock speed of at least 2.5 GHz is recommended for handling control logic tasks such as data preprocessing and tree structure construction.

Although Tucker decomposition significantly reduces the number of model parameters, intermediate results such as core tensors, factor matrices, and tree structure matrices still need to be stored during inference. When deploying on resource-constrained hardware, memory usage can be strictly controlled through techniques such as model quantization, dynamic batching, and gradient check-pointing. Converting the model from FP32 to FP16 can reduce memory usage by nearly half, while a block-based inference strategy can avoid memory overflows caused by processing large amounts of road network data all at once. For extremely resource-constrained scenarios, we recommend pre-computing the tree matrix and deploying the quantized model using inference engines such as TensorRT or OpenVINO to balance computational efficiency and resource consumption.

Overall, after proper optimization, FDSTTP can achieve efficient real-time inference on hardware with at least 4GB of video memory and 4GB of RAM. However, its performance scales linearly with network size and time step length, requiring flexible resource allocation adjustments based on the actual application scenario.

Complexity:
The core complexity characteristics of the FDSTTP model can be summarized as "compact structure and efficient computation." Its design strategy is to transform the high-order complexity of traditional graph neural networks, which is related to the number of nodes, into a low-order complexity related to the rank of a pre-set core tensor by introducing tree structure transformation and Tucker decomposition, thus achieving a qualitative leap in computational efficiency. In terms of time complexity, FDSTTP avoids the computationally expensive traditional graph convolution operations. Its overhead primarily stems from the iterative optimization process of Tucker decomposition, which has a complexity of approximately O(n x I x r3) (I: iteration), where the rank r of the core tensor is a fixed small value much smaller than the number of nodes n. Subsequent tree convolution and temporal convolution operations are performed on this extremely compressed low-dimensional core tensor. Their complexity depends only on r and the associated kernel size, completely avoiding any squared or higher-order dependencies on the number of nodes n. This makes its overall computational burden far lower than that of GCNs or attention mechanisms, which rely on large-scale adjacency matrix operations. In terms of space complexity, FDSTTP's advantage is even more pronounced. It completely avoids the significant overhead of storing an n x n adjacency matrix (O(n²)). The model's memory usage is primarily dominated by the compressed core tensor (O(rN)) and factor matrix (O(d x r), d: dimension), independent of the network size n. This design enables it to handle extremely large sensor networks without encountering memory bottlenecks.

Besides, during training, the standard deviations of MAE and RMSE were ±0.15 and ±0.11, respectively, indicating that the parameter optimization process is insensitive to initialization and the training process is stable. Additionally, after adopting a learning rate decay strategy, the model converged stably within 150 epochs, with the ratios of MAE/RMSE in the training phase to MAE/RMSE in the validation phase being 0.96 and 0.98 (close to 1).
